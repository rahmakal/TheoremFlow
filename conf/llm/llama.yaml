provider: "ollama"
model: "llama3"
temperature: 0.2
max_tokens: 4096
endpoint: "http://localhost:11434"
